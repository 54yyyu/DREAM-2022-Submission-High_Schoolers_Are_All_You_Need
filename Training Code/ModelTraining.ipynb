{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pgyVAi3by0Qo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pgyVAi3by0Qo",
    "outputId": "c44d887c-a32a-4ed8-9e50-961c37129e9b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.12.21)\n",
      "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
      "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.9)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.9.0)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.7/dist-packages (from wandb) (1.3.0)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
      "Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.27)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.1.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.6.15)\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.7/dist-packages (0.17.1)\n",
      "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow-addons) (3.0.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5aa728f",
   "metadata": {
    "id": "e5aa728f"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_addons as tfa\n",
    "import numpy as np\n",
    "import h5py\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbd73f7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cbbd73f7",
    "outputId": "475ab244-13cd-41b9-a35b-44e54b3b2a29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6600000, 111, 4) (104448, 111, 4) (71102, 111, 4)\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('compressed_dataset_111.h5', 'r') as hf:\n",
    "    x_train = np.array(hf['x_train'][:,:,:4]).astype(np.int8)\n",
    "    y_train = np.array(hf['y_train']).astype(np.float32)\n",
    "    \n",
    "x_valid = x_train[-104448:]\n",
    "x_train = x_train[:-104489]\n",
    "y_valid = y_train[-104448:]\n",
    "y_train = y_train[:-104489]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95476aea",
   "metadata": {
    "id": "95476aea"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention2(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, *args, embedding_size=None, **kwargs):\n",
    "        super(MultiHeadAttention2, self).__init__(*args, **kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.embedding_size = d_model if embedding_size == None else embedding_size\n",
    "\n",
    "        assert d_model % self.num_heads == 0 and d_model % 6 == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model, use_bias=False)\n",
    "        self.wk = tf.keras.layers.Dense(d_model, use_bias=False)\n",
    "        self.wv = tf.keras.layers.Dense(d_model, use_bias=False)\n",
    "        \n",
    "        self.r_k_layer = tf.keras.layers.Dense(d_model, use_bias=False)\n",
    "        self.r_w = tf.Variable(tf.random_normal_initializer(0, 0.5)(shape=[1, self.num_heads, 1, self.depth]), trainable=True, name=f'{self.name}-r_w')\n",
    "        self.r_r = tf.Variable(tf.random_normal_initializer(0, 0.5)(shape=[1, self.num_heads, 1, self.depth]), trainable=True, name=f'{self.name}-r_r')\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"d_model\": self.d_model,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            'embedding_size': self.embedding_size\n",
    "        })\n",
    "        return config\n",
    "        \n",
    "    def split_heads(self, x, batch_size, seq_len):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, seq_len, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, v, k, q):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        seq_len = tf.constant(q.shape[1])\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size, seq_len)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size, seq_len)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size, seq_len)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "        q = q / tf.math.sqrt(tf.cast(self.depth, dtype=tf.float32))\n",
    "        \n",
    "        pos = tf.range(-seq_len + 1, seq_len, dtype=tf.float32)[tf.newaxis]\n",
    "        feature_size=self.embedding_size//6\n",
    "\n",
    "        seq_length = tf.cast(seq_len, dtype=tf.float32)\n",
    "        exp1 = f_exponential(tf.abs(pos), feature_size, seq_length=seq_length)\n",
    "        exp2 = tf.multiply(exp1, tf.sign(pos)[..., tf.newaxis])\n",
    "        cm1 = f_central_mask(tf.abs(pos), feature_size, seq_length=seq_length)\n",
    "        cm2 = tf.multiply(cm1, tf.sign(pos)[..., tf.newaxis])\n",
    "        gam1 = f_gamma(tf.abs(pos), feature_size, seq_length=seq_length)\n",
    "        gam2 = tf.multiply(gam1, tf.sign(pos)[..., tf.newaxis])\n",
    "\n",
    "        # [1, 2seq_len - 1, embedding_size]\n",
    "        positional_encodings = tf.concat([exp1, exp2, cm1, cm2, gam1, gam2], axis=-1)\n",
    "        positional_encodings = tf.keras.layers.Dropout(0.1)(positional_encodings)\n",
    "        \n",
    "        # [1, 2seq_len - 1, d_model]\n",
    "        r_k = self.r_k_layer(positional_encodings)\n",
    "        \n",
    "        # [1, 2seq_len - 1, num_heads, depth]\n",
    "        r_k = tf.reshape(r_k, [r_k.shape[0], r_k.shape[1], self.num_heads, self.depth])\n",
    "        r_k = tf.transpose(r_k, perm=[0, 2, 1, 3])\n",
    "        # [1, num_heads, 2seq_len - 1, depth]\n",
    "        \n",
    "        # [batch_size, num_heads, seq_len, seq_len]\n",
    "        content_logits = tf.matmul(q + self.r_w, k, transpose_b=True)\n",
    "        \n",
    "        # [batch_size, num_heads, seq_len, 2seq_len - 1]\n",
    "        relative_logits = tf.matmul(q + self.r_r, r_k, transpose_b=True)\n",
    "        # [batch_size, num_heads, seq_len, seq_len]\n",
    "        relative_logits = relative_shift(relative_logits)\n",
    "        \n",
    "        # [batch_size, num_heads, seq_len, seq_len]\n",
    "        logits = content_logits + relative_logits\n",
    "        attention_map = tf.nn.softmax(logits)\n",
    "        \n",
    "        # [batch_size, num_heads, seq_len, depth]\n",
    "        attended_values = tf.matmul(attention_map, v)\n",
    "        # [batch_size, seq_len, num_heads, depth]\n",
    "        attended_values = tf.transpose(attended_values, perm=[0, 2, 1, 3])\n",
    "        \n",
    "        concat_attention = tf.reshape(attended_values, [batch_size, seq_len, self.d_model])\n",
    "        \n",
    "        output = self.dense(concat_attention)\n",
    "        \n",
    "        return output, attention_map\n",
    "\n",
    "\n",
    "def f_exponential(positions, feature_size, seq_length=None, min_half_life=3.0):\n",
    "    if seq_length is None:\n",
    "        seq_length = tf.cast(tf.reduce_max(tf.abs(positions)) + 1, dtype=tf.float32)\n",
    "    max_range = tf.math.log(seq_length) / tf.math.log(2.0)\n",
    "    half_life = tf.pow(2.0, tf.linspace(min_half_life, max_range, feature_size))\n",
    "    half_life = tf.reshape(half_life, shape=[1]*positions.shape.rank + half_life.shape)\n",
    "    positions = tf.abs(positions)\n",
    "    outputs = tf.exp(-tf.math.log(2.0) / half_life * positions[..., tf.newaxis])\n",
    "    return outputs\n",
    "\n",
    "def f_central_mask(positions, feature_size, seq_length=None):\n",
    "    center_widths = tf.pow(2.0, tf.range(1, feature_size + 1, dtype=tf.float32)) - 1\n",
    "    center_widths = tf.reshape(center_widths, shape=[1]*positions.shape.rank + center_widths.shape)\n",
    "    outputs = tf.cast(center_widths > tf.abs(positions)[..., tf.newaxis], tf.float32)\n",
    "    return outputs\n",
    "\n",
    "def f_gamma(positions, feature_size, seq_length=None):\n",
    "    if seq_length is None:\n",
    "        seq_length = tf.reduce_max(tf.abs(positions)) + 1\n",
    "    stdv = seq_length / (2*feature_size)\n",
    "    start_mean = seq_length / feature_size\n",
    "    mean = tf.linspace(start_mean, seq_length, num=feature_size)\n",
    "    mean = tf.reshape(mean, shape=[1]*positions.shape.rank + mean.shape)\n",
    "    concentration = (mean / stdv) ** 2\n",
    "    rate = mean / stdv**2\n",
    "    def gamma_pdf(x, conc, rt):\n",
    "        log_unnormalized_prob = tf.math.xlogy(concentration - 1., x) - rate * x\n",
    "        log_normalization = (tf.math.lgamma(concentration) - concentration * tf.math.log(rate))\n",
    "        return tf.exp(log_unnormalized_prob - log_normalization)\n",
    "    probabilities = gamma_pdf(tf.abs(tf.cast(positions, dtype=tf.float32))[..., tf.newaxis], concentration, rate)\n",
    "    outputs = probabilities / tf.reduce_max(probabilities)\n",
    "    return outputs\n",
    "    \n",
    "def relative_shift(x):\n",
    "    to_pad = tf.zeros_like(x[..., :1])\n",
    "    x = tf.concat([to_pad, x], -1)\n",
    "    _, num_heads, t1, t2 = x.shape\n",
    "    x = tf.reshape(x, [-1, num_heads, t2, t1])\n",
    "    x = tf.slice(x, [0, 0, 1, 0], [-1, -1, -1, -1])\n",
    "    x = tf.reshape(x, [-1, num_heads, t1, t2 - 1])\n",
    "    x = tf.slice(x, [0, 0, 0, 0], [-1, -1, -1, (t2 + 1) // 2])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1660b6cc",
   "metadata": {
    "id": "1660b6cc"
   },
   "outputs": [],
   "source": [
    "class RevCompConv1D(tf.keras.layers.Conv1D):\n",
    "    \"\"\"\n",
    "    Implement forward and reverse-complement filter convolutions\n",
    "    for 1D signals. It takes as input either a single input or two inputs \n",
    "    (where the second input is the reverse complement scan). If a single input, \n",
    "    this performs both forward and reverse complement scans and either merges it \n",
    "    (if concat=True) or returns a separate scan for forward and reverse comp. \n",
    "    \"\"\"\n",
    "    def __init__(self, *args, concat=True, **kwargs):\n",
    "        super(RevCompConv1D, self).__init__(*args, **kwargs)\n",
    "        self.concat = concat\n",
    "\n",
    "\n",
    "    def call(self, inputs, inputs2=None):\n",
    "\n",
    "        if inputs2 is not None:\n",
    "            # create rc_kernels\n",
    "            rc_kernel = self.kernel[::-1,::-1,:]\n",
    "\n",
    "            # convolution 1D\n",
    "            outputs = self.convolution_op(inputs, self.kernel)\n",
    "            rc_outputs = self.convolution_op(inputs2, rc_kernel)\n",
    "\n",
    "        else:\n",
    "            # create rc_kernels\n",
    "            rc_kernel = tf.concat([self.kernel, self.kernel[::-1,:,:][:,::-1,:]], axis=-1)\n",
    "\n",
    "            # convolution 1D\n",
    "            outputs = self.convolution_op(inputs, rc_kernel)\n",
    "\n",
    "            # unstack to forward and reverse strands\n",
    "            outputs = tf.unstack(outputs, axis=2)\n",
    "            rc_outputs = tf.stack(outputs[self.filters:], axis=2)\n",
    "            outputs = tf.stack(outputs[:self.filters], axis=2)\n",
    "\n",
    "        # add bias\n",
    "        if self.use_bias:\n",
    "            outputs = tf.nn.bias_add(outputs, self.bias)\n",
    "            rc_outputs = tf.nn.bias_add(rc_outputs, self.bias)\n",
    "\n",
    "        # add activations\n",
    "        if self.activation is not None:\n",
    "            outputs = self.activation(outputs)\n",
    "            rc_outputs = self.activation(rc_outputs)\n",
    "\n",
    "        if self.concat:\n",
    "            return tf.concat([outputs, rc_outputs], axis=-1)\n",
    "        else:\n",
    "            return outputs, rc_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaf55a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block(input_layer, kernel_size=3, activation='relu', num_layers=5, dropout=0.1):\n",
    "\n",
    "    filters = input_layer.shape.as_list()[-1]  \n",
    "\n",
    "    nn = tf.keras.layers.Conv1D(filters=filters,\n",
    "                           kernel_size=kernel_size,\n",
    "                           activation=None,\n",
    "                           use_bias=False,\n",
    "                           padding='same',\n",
    "                           dilation_rate=1)(input_layer) \n",
    "    nn = tf.keras.layers.BatchNormalization()(nn)\n",
    "\n",
    "    base_rate = 2\n",
    "    for i in range(1,num_layers):\n",
    "        nn = tf.keras.layers.Activation('relu')(nn)\n",
    "        nn = tf.keras.layers.Dropout(dropout)(nn)\n",
    "        nn = tf.keras.layers.Conv1D(filters=filters,\n",
    "                                 kernel_size=kernel_size,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 dilation_rate=base_rate**i)(nn) \n",
    "        nn = tf.keras.layers.BatchNormalization()(nn)\n",
    "    nn = tf.keras.layers.Add()([input_layer, nn])\n",
    "    return tf.keras.layers.Activation(activation)(nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZINiQBv3vuRm",
   "metadata": {
    "id": "ZINiQBv3vuRm"
   },
   "outputs": [],
   "source": [
    "class ProfileModel(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, bin_size=1, rc_prob=0.5, *args, **kwargs):\n",
    "        super(ProfileModel, self).__init__(*args, **kwargs)\n",
    "        self.bin_size = bin_size\n",
    "        self.rc_prob = rc_prob\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(ProfileModel, self).get_config()\n",
    "        config.update({\"bin_size\": self.bin_size})\n",
    "        config.update({'rc_prob': self.rc_prob})\n",
    "        return config\n",
    "\n",
    "    def train_step(self, data):\n",
    "        if len(data) == 3:\n",
    "            x, y, sample_weight = data\n",
    "        else:\n",
    "            sample_weight = None\n",
    "            x, y = data\n",
    "\n",
    "        # online target resolution calculation\n",
    "        y = bin_resolution(y, self.bin_size)\n",
    "\n",
    "        # stochastic reverse complement\n",
    "        x, y = reverse_complement(x, y, p=self.rc_prob)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True)  \n",
    "            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "\n",
    "    def test_step(self, data):\n",
    "        if len(data) == 3:\n",
    "            x, y, sample_weight = data\n",
    "        else:\n",
    "            sample_weight = None\n",
    "            x, y = data\n",
    "\n",
    "\n",
    "        # online target resolution calculation\n",
    "        y = bin_resolution(y, self.bin_size)\n",
    "\n",
    "        y_pred = self(x, training=False)\n",
    "        x_RC, y_RC = reverse_complement(x, y, p=1.0)\n",
    "        y_pred_RC = self(x_RC, training=False)\n",
    "        y_pred = tf.math.reduce_mean([y_pred, y_pred_RC], axis=0)\n",
    "        self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
    "        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "    \n",
    "\"\"\"    def compute_loss(self, x, y, y_pred, sample_weight):\n",
    "        del x  # The default implementation does not use `x`.\n",
    "        return self.compiled_loss(\n",
    "            y, y_pred, sample_weight, regularization_losses=self.losses)\"\"\"\n",
    "    \n",
    "\n",
    "def reverse_complement(x, y, p=0.5):\n",
    "    # x_rc = tf.gather(x, [3, 2, 1, 0], axis=-1)\n",
    "    x_rc = tf.reverse(x, axis=[2])\n",
    "    x_rc = tf.reverse(x_rc, axis=[1])\n",
    "    # y_rc = tf.reverse(y, axis=[1])\n",
    "    switch = tf.random.uniform(shape=[]) > (1 - p)\n",
    "    x_new = tf.cond(switch, lambda: x_rc, lambda: x)\n",
    "    # y_new = tf.cond(switch, lambda: y_rc, lambda: y)\n",
    "    return x_new, y\n",
    "\n",
    "def bin_resolution(y, bin_size):\n",
    "    if bin_size > 1:\n",
    "        y_dim = tf.shape(y)\n",
    "        num_bins = tf.cast(y_dim[1] / bin_size, 'int32')\n",
    "        y_reshape = tf.reshape(y, (y_dim[0], num_bins, bin_size, y_dim[2]))\n",
    "        y_bin = tf.math.reduce_mean(y_reshape, axis=2)\n",
    "        return y_bin\n",
    "    else:\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b451ddc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionPooling(keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, pool_size, *args, **kwargs):\n",
    "        super(AttentionPooling, self).__init__(*args, **kwargs)\n",
    "        self.pool_size = pool_size\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"pool_size\": self.pool_size,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.dense = keras.layers.Conv1D(filters=input_shape[-1], kernel_size=1, activation=None, use_bias=False)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        N, L, F = inputs.shape\n",
    "        inputs = tf.keras.layers.Cropping1D((0, L % self.pool_size))(inputs)\n",
    "        inputs = tf.reshape(inputs, (-1, L//self.pool_size, self.pool_size, F))\n",
    "\n",
    "        raw_weights = self.dense(inputs)\n",
    "        att_weights = tf.nn.softmax(raw_weights, axis=-2)\n",
    "        \n",
    "        return tf.math.reduce_sum(inputs * att_weights, axis=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OpANHeqHDUsx",
   "metadata": {
    "id": "OpANHeqHDUsx"
   },
   "outputs": [],
   "source": [
    "inputs = keras.layers.Input(shape=(111,4))\n",
    "\n",
    "nn = RevCompConv1D(filters=192, kernel_size=19, padding='same', concat=True)(inputs)\n",
    "nn = keras.layers.BatchNormalization()(nn)\n",
    "nn = keras.layers.Activation('exponential')(nn)\n",
    "nn = residual_block(nn, kernel_size=3, activation='relu', num_layers=4, dropout=0.1)\n",
    "nn = keras.layers.Dropout(0.2)(nn)\n",
    "nn = AttentionPooling(4)(nn)\n",
    "\n",
    "mod = ProfileModel(inputs=inputs, outputs=nn)\n",
    "nn = mod(inputs)\n",
    "hold = nn\n",
    "\n",
    "nn = keras.layers.Conv1D(filters=256, kernel_size=9, use_bias=True, padding='same', activation='relu')(nn)\n",
    "nn = keras.layers.BatchNormalization()(nn)\n",
    "nn = keras.layers.Activation('relu')(nn)\n",
    "nn = residual_block(nn, kernel_size=3, activation='relu', num_layers=2, dropout=0.1)\n",
    "nn = AttentionPooling(4)(nn)\n",
    "nn = keras.layers.Dropout(0.3)(nn)\n",
    "\n",
    "mod = ProfileModel(inputs=hold, outputs=nn)\n",
    "nn = mod(hold)\n",
    "hold = nn\n",
    "\n",
    "nn = keras.layers.Conv1D(filters=192, kernel_size=7, padding='same')(nn)\n",
    "nn = keras.layers.BatchNormalization()(nn)\n",
    "nn = keras.layers.Activation('relu')(nn)\n",
    "nn = residual_block(nn, kernel_size=3, activation='relu', num_layers=2, dropout=0.1)\n",
    "nn = AttentionPooling(2)(nn)\n",
    "nn = keras.layers.Dropout(0.1)(nn)\n",
    "\n",
    "mod = ProfileModel(inputs=hold, outputs=nn)\n",
    "nn = mod(hold)\n",
    "hold = nn\n",
    "\n",
    "nn1, att = MultiHeadAttention2(num_heads=8, d_model=192)(nn, nn, nn)\n",
    "nn1 = keras.layers.Dropout(0.1)(nn1)\n",
    "nn = tf.add(nn, nn1)\n",
    "nn = keras.layers.LayerNormalization()(nn)\n",
    "nn1 = keras.layers.Conv1D(filters=1024, kernel_size=1)(nn)\n",
    "nn1 = keras.layers.Dropout(0.1)(nn1)\n",
    "nn1 = keras.layers.Activation('relu')(nn1)\n",
    "nn1 = keras.layers.Conv1D(filters=192, kernel_size=1)(nn1)\n",
    "nn1 = keras.layers.Dropout(0.1)(nn1)\n",
    "nn = tf.add(nn, nn1)\n",
    "nn = keras.layers.LayerNormalization()(nn)\n",
    "\n",
    "nn = keras.layers.Flatten()(nn)\n",
    "\n",
    "nn = keras.layers.Dense(256)(nn)\n",
    "nn = keras.layers.BatchNormalization()(nn)\n",
    "nn = keras.layers.Activation('relu')(nn)\n",
    "nn = keras.layers.Dropout(0.4)(nn)\n",
    "\n",
    "nn = keras.layers.Dense(256)(nn)\n",
    "nn = keras.layers.BatchNormalization()(nn)\n",
    "nn = keras.layers.Activation('relu')(nn)\n",
    "nn = keras.layers.Dropout(0.4)(nn)\n",
    "\n",
    "outputs = keras.layers.Dense(1, activation='linear')(nn)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "opt = tfa.optimizers.SWA(tf.keras.optimizers.Adam(1e-4), start_averaging=150, average_period=30)\n",
    "model.compile(optimizer=opt, loss='MSE', metrics=[])\n",
    "\n",
    "prs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "N2NjVsrXYWhj",
   "metadata": {
    "id": "N2NjVsrXYWhj"
   },
   "outputs": [],
   "source": [
    "probabilities, bins, _ = plt.hist(y_train, bins=10)\n",
    "plt.close()\n",
    "bins = bins[:-1]\n",
    "probabilities = probabilities/np.sum(probabilities)\n",
    "p_dist = np.array([bins, probabilities])\n",
    "\n",
    "# loss_func = RegressionLoss(bins)\n",
    "loss_func = 'MSE'\n",
    "lr = 0.0001\n",
    "opt = tfa.optimizers.SWA(tf.keras.optimizers.Adam(lr), start_averaging=150, average_period=30)\n",
    "model.compile(optimizer=opt, loss=loss_func, metrics=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wQnwhqCmOlfe",
   "metadata": {
    "id": "wQnwhqCmOlfe"
   },
   "outputs": [],
   "source": [
    "# If training in more than one run\n",
    "\n",
    "model = tf.keras.models.load_model('model_location-h5', custom_objects={\n",
    "    'ProfileModel' : ProfileModel,\n",
    "    'MultiHeadAttention2' : MultiHeadAttention2,\n",
    "    'RevCompConv1D' : RevCompConv1D,\n",
    "    'AttentionPooling' : AttentionPooling\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8nkbYl2lkaA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d8nkbYl2lkaA",
    "outputId": "e1f3e900-038c-4c23-9559-5e6221a97266"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 751\n",
      " 41/300 [===>..........................] - ETA: 9s - loss: 2.4779"
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "dataset_size = x_train.shape[0]\n",
    "slice_size = 307200\n",
    "\n",
    "for i in range(750):\n",
    "    print('Epoch %d' % (i+1))\n",
    "    \n",
    "    perm = np.random.choice(dataset_size, size=slice_size, replace=False)\n",
    "    model.fit(x_train[perm], y_train[perm], epochs=1, verbose=1, batch_size=batch_size)\n",
    "\n",
    "    if i % 3 == 0:\n",
    "        y_pred = np.squeeze(model.predict(x_valid, batch_size=1024))\n",
    "        pr = stats.pearsonr(y_pred, y_valid)[0]\n",
    "        sr = stats.spearmanr(y_pred, y_valid)[0]\n",
    "        ls = tf.keras.losses.MSE(y_pred, y_valid).numpy()\n",
    "        \n",
    "        if pr >= np.max(prs) - 1e-7:\n",
    "            model.save(f'/content/drive/MyDrive/Colab Notebooks/DREAM/models/{wb.run.name}_3.h5')\n",
    "            \n",
    "        prs.append(pr)\n",
    "        \n",
    "        print('pearson r:', pr)\n",
    "        print('spearman r:', sr)\n",
    "        print('valid MSE:', ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-cy5uudwXAsi",
   "metadata": {
    "id": "-cy5uudwXAsi"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.set_value(model.optimizer.learning_rate, 3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854627f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "dataset_size = x_train.shape[0]\n",
    "slice_size = 307200\n",
    "\n",
    "for i in range(750, 1250):\n",
    "    print('Epoch %d' % (i+1))\n",
    "    \n",
    "    perm = np.random.choice(dataset_size, size=slice_size, replace=False)\n",
    "    model.fit(x_train[perm], y_train[perm], epochs=1, verbose=1, batch_size=batch_size)\n",
    "\n",
    "    if i % 3 == 0:\n",
    "        y_pred = np.squeeze(model.predict(x_valid, batch_size=1024))\n",
    "        pr = stats.pearsonr(y_pred, y_valid)[0]\n",
    "        sr = stats.spearmanr(y_pred, y_valid)[0]\n",
    "        ls = tf.keras.losses.MSE(y_pred, y_valid).numpy()\n",
    "        \n",
    "        if pr >= np.max(prs) - 1e-7:\n",
    "            model.save(f'/content/drive/MyDrive/Colab Notebooks/DREAM/models/{wb.run.name}_3.h5')\n",
    "            \n",
    "        prs.append(pr)\n",
    "        \n",
    "        print('pearson r:', pr)\n",
    "        print('spearman r:', sr)\n",
    "        print('valid MSE:', ls)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of NotEnsemble(1).ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
